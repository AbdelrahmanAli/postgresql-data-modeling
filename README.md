# Data Modeling with PostgreSQL

## Table of contents
- [Introduction](#introduction)
- [Project Description](#project-description)
- [Project Datasets](#project-datasets)
  * [Song dataset](#song-dataset)
  * [Log dataset](#log-dataset)
- [Technologies](#technologies)
- [Files](#files)
- [Setup](#setup)
- [Database Purpose](#database-purpose)
- [Schema Design](#schema-design)
- [Example For Song Play Analysis Queries](#example-for-song-play-analysis-queries)
  * [Top 10 Songs](#top-10-songs)
  * [Top 10 Artists](#top-10-artists)
  * [Top 5 active hours of the day](#top-5-active-hours-of-the-day)

## Introduction
This project is one of Udacity's Data Engineering Nano Degree projects\, it is requested in the 1st course: ***Data Modeling***.

You are required to build a simple ETL pipeline that transfers data from files in two local directories into a set of facts/dimensions tables representing a star in Postgres using Python and SQL.

## Project Description
>A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

>They'd like a data engineer to create a Postgres database with tables designed to optimize queries on song play analysis, and bring you on the project. Your role is to create a database schema and ETL pipeline for this analysis. You'll be able to test your database and ETL pipeline by running queries given to you by the analytics team from Sparkify and compare your results with their expected results.

## Project Datasets
There are two datasets residing in Local Directory:

- **Song data:** `data/song_data`
- **Log data:** `data/log_data`

### Song dataset
- Subset of [Million Song Dataset](http://millionsongdataset.com/)
- JSON files
- Sample:

```{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}```

### Log dataset
- Generated by [Event Simulator](https://github.com/Interana/eventsim)
- JSON files
- Sample:

![Log dataset sample](assets/log-data.png "Log Dataset Sample")

## Technologies
- Python 3
- Jupyter Notebook
- PostgreSql

## Files
 - `sql_queries.py`: Contains table drop\, creation, selection and insertion queries
 - `create_tables.py`: Connects to PostgreSQL\, drop all tables if exist\, then execute creation queries
 - `etl.py`: 
  - Connects to PostgreSQL and execute the insertion queries. 
  - It uses Pandas library in Python which provides the Dataframe API that facilitate applying  transformations on the data. 
 - `Analytics.ipynb`: Connectes to PostgreSQL and execute some analytics queries

## Setup
- Make sure you have `Python, Pandas, PostgreSql and Jupyter Notebook` installed and configured
- Configure PostgreSql connection in `etl.py` and `create_tables.py`
- Run `python3 create_tables.py`
- Run `etl.py`
- Open `Analytics.ipynb` and do your analytics

## Database Purpose

Scattered logs and meta-data in files makes the process of analyzing these files difficult and not an efficient process. JSON might be a good way to represent data in a structured way that is easy to read\, but it get complicated when there are many JSON files.   
   
A database will give a more organized way to represent the available data. Anyone can do adhoc queries on the data and retrieve the requried information rapidly. Also analytics is now possible for Sparkify analytics team.    
   
Since Sparkify still a small startup\, then the data it has is not big. A Relational database, like PostgreSQL, is a good choice to model the company data in an organized way.   
   
## Schema Design   

The analytics team in Sparkify wants to analyze the data in order to know what songs users are listening to. That's why a star schema is a good choice for this case. We can perform fast analytical queries on the fact table\, which is **songplays** in our case here. The star schema also results in reducing the number of JOINS required to perform queries.   
   
## Example For Song Play Analysis Queries

### Top 10 Songs
    %sql SELECT t2.title, count(*) 
    FROM songplays AS t1 
    JOIN songs AS t2 ON t1.song_id = t2.song_id 
    GROUP BY t2.title 
    ORDER BY count(*) DESC 
    LIMIT 10;

### Top 10 Artists
    %sql SELECT t2.name, count(*) 
    FROM songplays AS t1 
    JOIN artists AS t2 ON t1.artist_id = t2.artist_id 
    GROUP BY t2.name 
    ORDER BY count(*) DESC 
    LIMIT 10;
    
### Top 5 active hours of the day
    %sql SELECT t2.hour, count(*) 
    FROM songplays AS t1 
    JOIN time AS t2 ON t1.start_time = t2.start_time 
    GROUP BY t2.hour 
    ORDER BY count(*) DESC 
    LIMIT 5;


